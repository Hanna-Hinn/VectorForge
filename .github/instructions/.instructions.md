# AI Instructions for VectorForge

## Prime Directive: **NEVER ASSUME**

If anything is missing, ambiguous, contradictory, or unclear:
1. **Stop immediately**
2. **Ask targeted questions** before acting
3. **Do not guess, invent, or fabricate**

---

## Project Overview

**VectorForge** is a high-performance, standalone Retrieval-Augmented Generation (RAG) engine.

- **Language**: Python
- **License**: MIT (Hanna-Hinn)
- **Status**: Greenfield — the codebase is being actively built from scratch

### Technology Stack

| Layer | Technology | Notes |
|-------|-----------|-------|
| **Language** | Python 3.11+ | Type-hinted, async-capable |
| **Database** | PostgreSQL | Primary persistence layer |
| **Vector Store** | pgvector | PostgreSQL extension for embeddings |
| **Architecture** | Monolithic, standalone | Single deployable unit |
| **Frontend** | React (future) | Will be added later — do not build yet unless asked |
| **Testing** | pytest | With coverage, fixtures, mocking |
| **Linting** | Ruff | Check + format |
| **Type Checking** | mypy | Strict mode preferred |

### Dependency Mapping (Mandatory)

Before suggesting any package, check if the project already has an equivalent. Use these categories:

| Category | Preferred | Do NOT use |
|----------|-----------|------------|
| **ORM / DB** | SQLAlchemy 2.0+ / asyncpg | Raw SQL strings, Django ORM |
| **Validation** | Pydantic v2 | marshmallow, attrs for validation |
| **HTTP Client** | httpx (async) | requests (unless sync-only needed) |
| **Vector Ops** | pgvector + SQLAlchemy | pinecone, weaviate, chromadb (unless explicitly asked) |
| **Embeddings** | Provider SDKs (OpenAI, etc.) or sentence-transformers | Custom embedding code |
| **CLI** | click or typer | argparse for complex CLIs |
| **Logging** | stdlib `logging` | loguru, structlog (unless adopted) |
| **Date/Time** | stdlib `datetime` | pendulum, arrow |
| **Config** | pydantic-settings or python-dotenv | custom config parsers |

**Fallback Rule**: If a needed utility is not listed above, implement a lightweight custom function using stdlib rather than suggesting a new install. If a dependency is genuinely needed, justify it and ask first.

---

## Codebase-First Behavior

- **Always inspect the repository first** before answering or writing code
- **Cite exact file paths and symbols** you rely on (e.g., `vectorforge/retriever/dense.py#search`)
- When proposing or changing features, **strictly follow existing design, architecture, naming, and folder structure**
- **Always find similar features** in the repo and align with them
- If the codebase is empty or a section doesn't exist yet, **ask about the intended design before creating it**

---

## Question-First Protocol

### When Request Lacks Context, Respond With:

1. **My current understanding**: 1–3 lines summarizing what you think is asked
2. **Blocking questions**: Numbered list of what you need:
   - Which module or component is affected
   - Expected inputs/outputs and data formats
   - Target vector store / LLM provider
   - Embedding model and dimensions
   - Chunking strategy preferences
   - Error handling requirements
   - Performance constraints (latency, throughput)
   - Persistence and storage requirements
   - Database schema implications
3. **If confirmed**: Concise plan of steps you will take next

### Do NOT write code or fabricate APIs/paths/configs until blocking questions are answered.

---

## Repository Structure

> **This section must be kept up to date** as the project grows. After adding new top-level directories or major modules, update this tree.

```
VectorForge/
├── .github/
│   └── instructions/
│       └── .instructions.md      # This file — AI behavior rules
├── .gitignore
├── LICENSE
└── README.md
```

<!-- As modules are added, expand the tree here. Example of target layout:
VectorForge/
├── vectorforge/                   # Core package
│   ├── __init__.py
│   ├── core/                      # Core engine logic
│   ├── chunking/                  # Document chunking strategies
│   ├── embedding/                 # Embedding model integrations
│   ├── vectorstore/               # Vector store backends (pgvector)
│   ├── retriever/                 # Retrieval strategies
│   ├── llm/                       # LLM provider integrations
│   ├── pipeline/                  # RAG pipeline orchestration
│   ├── db/                        # Database layer (PostgreSQL, migrations)
│   ├── api/                       # REST API layer (when added)
│   ├── models/                    # Pydantic / SQLAlchemy models
│   ├── config/                    # Configuration management
│   └── utils/                     # Shared utilities
├── tests/                         # Test suite
├── docs/                          # Documentation
├── examples/                      # Usage examples
├── migrations/                    # Database migrations (Alembic)
├── pyproject.toml                 # Project metadata & dependencies
└── README.md
-->

---

## Development Workflows

### Python Environment

**Always inspect project configuration files** (`pyproject.toml`, `setup.py`, `setup.cfg`, `requirements.txt`, `Makefile`) before running commands.

```bash
# Check for project config
cat pyproject.toml          # Linux/macOS
Get-Content pyproject.toml  # Windows PowerShell

# Virtual environment
python -m venv .venv
.venv\Scripts\activate      # Windows
source .venv/bin/activate   # Linux/macOS

# Install dependencies (verify which tool the project uses first)
pip install -e ".[dev]"     # Editable install with dev extras
pip install -r requirements.txt  # If requirements file exists

# Standard development workflow
pip install -e ".[dev]" && python -m pytest && python -m ruff check .
```

### Testing

```bash
# Run full test suite
python -m pytest

# Run with coverage
python -m pytest --cov=vectorforge --cov-report=term-missing

# Run specific test file or test
python -m pytest tests/test_chunking.py
python -m pytest tests/test_chunking.py::test_specific_case -v
```

### Linting & Formatting

```bash
# Linting (verify which linter the project uses)
python -m ruff check .
python -m ruff check . --fix

# Formatting
python -m ruff format .

# Type checking
python -m mypy vectorforge/
```

### Database

```bash
# PostgreSQL + pgvector (verify connection config in project settings)
# Migrations (if using Alembic)
alembic upgrade head
alembic revision --autogenerate -m "description"

# Verify pgvector extension
psql -c "CREATE EXTENSION IF NOT EXISTS vector;"
```

---

## SOLID Compliance (Strictly Enforced)

Every service-level class and module **must** be evaluated against these principles:

### S — Single Responsibility Principle

- Does this service or class focus on a **single, well-defined job**?
- If a class has multiple reasons to change, **split it**
- One module = one purpose. A `ChunkingService` should not also handle embeddings

### O — Open/Closed Principle

- Code should be **open for extension, closed for modification**
- Use abstract base classes / Protocols to define extension points
- New embedding providers, vector stores, or chunking strategies should be addable **without touching existing code**

```python
# GOOD: Open for extension
class BaseEmbedder(ABC):
    @abstractmethod
    def embed(self, texts: list[str]) -> list[list[float]]: ...

class OpenAIEmbedder(BaseEmbedder):
    def embed(self, texts: list[str]) -> list[list[float]]:
        # OpenAI-specific implementation
        ...

# BAD: Closed to extension
def get_embeddings(texts, provider="openai"):
    if provider == "openai":
        ...
    elif provider == "cohere":  # Must modify existing code to add providers
        ...
```

### L — Liskov Substitution Principle

- Any subtype must be **substitutable** for its parent without breaking behavior
- If `PgVectorStore` extends `BaseVectorStore`, it must honor the full contract
- Do not override methods in ways that violate the base class's promises

### I — Interface Segregation Principle

- Do not force clients to depend on methods they don't use
- Split large interfaces into focused ones
- Example: separate `Searchable` and `Indexable` protocols instead of one giant `VectorStore` interface

### D — Dependency Inversion Principle

- High-level modules depend on **abstractions**, not concrete implementations
- The pipeline orchestrator should depend on `BaseRetriever`, not `PgVectorRetriever`
- Use dependency injection (constructor parameters) to wire implementations

---

## Clean Code Standards

> **Be concise, direct, and solution-focused.** Write code directly — don't write tutorials.

### Naming Rules

| Element | Convention | Example |
|---------|-----------|---------|
| **Variables** | Reveal intent | `user_count` not `n` |
| **Functions** | Verb + noun | `get_user_by_id()` not `user()` |
| **Booleans** | Question form | `is_active`, `has_permission`, `can_edit` |
| **Constants** | SCREAMING_SNAKE | `MAX_RETRY_COUNT`, `DEFAULT_CHUNK_SIZE` |
| **Classes** | PascalCase, noun | `DocumentChunker`, `EmbeddingService` |
| **Private** | Leading underscore | `_internal_helper()`, `_cache` |

> **Rule**: If you need a comment to explain a name, rename it.

### Function Rules

| Rule | Description |
|------|-------------|
| **Small** | Max 20 lines, ideally 5–10 |
| **One Thing** | Does one thing, does it well |
| **One Level** | One level of abstraction per function |
| **Few Args** | Max 3 arguments, prefer 0–2. Use dataclasses/Pydantic for more |
| **No Side Effects** | Don't mutate inputs unexpectedly |
| **Guard Clauses** | Early returns for edge cases — flat over nested |

### Code Structure

| Pattern | Apply |
|---------|-------|
| **Guard Clauses** | Early returns for edge cases |
| **Flat > Nested** | Avoid deep nesting (max 2 levels) |
| **Composition** | Small functions composed together |
| **Colocation** | Keep related code close |

### Anti-Patterns (NEVER Do These)

| Bad Pattern | Fix |
|-------------|-----|
| Comment every line | Delete obvious comments |
| Helper for a one-liner | Inline the code |
| Factory for 2 objects | Direct instantiation |
| `utils.py` with 1 function | Put code where it's used |
| Deep nesting | Guard clauses + early returns |
| Magic numbers | Named constants |
| God functions (100+ lines) | Split by responsibility |
| Bare `except:` | Catch specific exceptions |
| `print()` for logging | Use `logging` module |
| `# type: ignore` everywhere | Fix the types |

### Before Editing ANY File (THINK FIRST)

| Question | Why |
|----------|-----|
| **What imports this file?** | They might break |
| **What does this file import?** | Interface changes |
| **What tests cover this?** | Tests might fail |
| **Is this a shared module?** | Multiple places affected |

> **Rule**: Edit the file + all dependent files in the SAME task. Never leave broken imports or missing updates.

### Self-Check Before Completing (MANDATORY)

| Check | Question |
|-------|----------|
| **Goal met?** | Did I do exactly what was asked? |
| **Files edited?** | Did I modify all necessary files? |
| **Code works?** | Did I test/verify the change? |
| **No errors?** | Lint, mypy, and pytest pass? |
| **Nothing forgotten?** | Any edge cases missed? |

> **Rule**: If ANY check fails, fix it before completing.

---

## Implementation Rules

### Keep It Simple

- **Reuse patterns** — look at the existing codebase and follow the style
- **Clear single-purpose functions** with clear naming
- **Avoid complex flows** — follow SRP. One method/class does one thing well
- Each file should have a single purpose that is **clean, understandable, meaningful**
- If a 10-line function can replace a complex class without losing readability, **use the function**

### DRY Principle

- **Reuse code** instead of duplicating it
- Build **shared abstractions** (base classes, protocols) for common patterns
- For third-party integrations, **always isolate them in their own module** under the appropriate sub-package
- Identify logic duplicated at the service level and **extract it**

### Complexity Reduction

- Prefer simple functions over unnecessary class hierarchies
- If a pattern adds indirection without value, skip it
- Don't abstract until you have 2+ concrete implementations (Rule of Three)
- Flat module structure over deep nesting

### Code Quality Standards

- **Clean, maintainable code** with docstrings (Google or NumPy style — stay consistent with whatever the codebase adopts)
- **Type hints everywhere** — avoid `Any` unless absolutely necessary
- **Follow existing linting/formatting** rules
- **Minimal, reversible diffs** — don't refactor unrelated code in the same change

### Dependency Discipline

- **Do NOT add new dependencies** without explicit approval
- Prefer the Python standard library when possible
- When a dependency is needed, justify it clearly (why not stdlib? why this package over alternatives?)
- All dependencies must be declared in `pyproject.toml` (or the project's dependency file)

### Testing Requirements

- **Always update existing tests** when modifying features
- **Add tests for new features** — prefer updating existing test files over creating atomic ones
- **Match the repository's test framework** and patterns (pytest expected)
- **Aim for meaningful coverage** — test edge cases, error paths, and integration points
- **Mock external services** (LLM APIs, vector stores, database) in unit tests
- Check: correctness, type safety, error handling, performance, security, documentation

### Documentation Requirements

- **Always update existing documentation** when modifying features
- **Add documentation for new features** — docstrings, README sections, and usage examples
- **Write documentation so a new developer can understand the system quickly**

---

## Python Expert Standards

### Development Priority Order

Follow this priority when writing or reviewing Python code:

1. **Correctness** (CRITICAL) — Logic errors, edge cases, boundary conditions
2. **Type Safety** (HIGH) — Complete type hints, correct types, consistency
3. **Performance** (HIGH) — Efficient algorithms, no unnecessary computation
4. **Style & Documentation** (MEDIUM) — PEP 8, docstrings, clear naming

### Type Hints (Mandatory)

```python
from typing import Optional
from collections.abc import Sequence

def search_documents(
    query: str,
    limit: int = 10,
    filters: dict[str, str] | None = None,
) -> list[dict[str, any]]:
    """Search documents by semantic similarity.

    Args:
        query: The search query text.
        limit: Maximum number of results to return.
        filters: Optional metadata filters to apply.

    Returns:
        List of matching documents with scores.

    Raises:
        ValueError: If limit is less than 1.
        ConnectionError: If vector store is unreachable.
    """
    if limit < 1:
        raise ValueError(f"limit must be >= 1, got {limit}")
    ...
```

### Correctness Rules

- **Never use mutable default arguments** (`def f(items=[])` — use `None` and assign inside)
- **Always handle edge cases**: empty collections, None values, boundary conditions
- **Use specific exceptions**: `ValueError`, `TypeError`, `ConnectionError` — never bare `except:`
- **Use context managers** for resource management (`with` statements for files, DB connections, locks)

### Performance Guidelines

- **Prefer comprehensions** over explicit loops where readable
- **Use generators** for large data streams (`yield` instead of building full lists)
- **Leverage stdlib**: `collections.Counter`, `itertools`, `functools.lru_cache`
- **Batch database and API calls** — never loop single queries
- Profile before optimizing — don't prematurely optimize

### Docstring Format

Use **Google style** consistently:

```python
def chunk_document(
    text: str,
    chunk_size: int = 512,
    overlap: int = 50,
) -> list[str]:
    """Split a document into overlapping chunks.

    Uses recursive character splitting with configurable
    chunk size and overlap for optimal retrieval.

    Args:
        text: The full document text to chunk.
        chunk_size: Target size of each chunk in characters.
        overlap: Number of overlapping characters between chunks.

    Returns:
        List of text chunks ready for embedding.

    Raises:
        ValueError: If chunk_size <= overlap.

    Example:
        >>> chunks = chunk_document("Hello world. Foo bar.", chunk_size=10, overlap=2)
        >>> len(chunks) > 0
        True
    """
```

---

## Backend Architecture Patterns

> Adapted for Python + PostgreSQL + pgvector monolith.

### Layered Architecture

```
vectorforge/
├── api/                # HTTP layer (routes, request/response handling)
│   ├── routes/         # Endpoint definitions
│   └── middleware/     # Auth, logging, rate limiting
├── services/           # Business logic layer
├── repositories/       # Data access layer (DB queries)
├── models/             # Domain models (Pydantic + SQLAlchemy)
├── external/           # Third-party integrations (LLM providers, etc.)
└── config/             # Configuration and settings
```

### Repository Pattern

```python
from abc import ABC, abstractmethod

class BaseDocumentRepository(ABC):
    """Abstract data access — swap implementations without touching services."""

    @abstractmethod
    async def find_by_id(self, doc_id: str) -> Document | None: ...

    @abstractmethod
    async def find_all(self, filters: DocumentFilters | None = None) -> list[Document]: ...

    @abstractmethod
    async def create(self, data: CreateDocumentDTO) -> Document: ...

    @abstractmethod
    async def delete(self, doc_id: str) -> None: ...


class PgDocumentRepository(BaseDocumentRepository):
    """PostgreSQL implementation of document repository."""

    def __init__(self, session: AsyncSession) -> None:
        self._session = session

    async def find_by_id(self, doc_id: str) -> Document | None:
        result = await self._session.execute(
            select(DocumentModel).where(DocumentModel.id == doc_id)
        )
        return result.scalar_one_or_none()
```

### Service Layer Pattern

```python
class DocumentService:
    """Business logic — separated from data access and HTTP concerns."""

    def __init__(
        self,
        repo: BaseDocumentRepository,
        embedder: BaseEmbedder,
        vector_store: BaseVectorStore,
    ) -> None:
        self._repo = repo
        self._embedder = embedder
        self._vector_store = vector_store

    async def ingest(self, text: str, metadata: dict) -> Document:
        chunks = chunk_document(text)
        embeddings = await self._embedder.embed(chunks)
        doc = await self._repo.create(CreateDocumentDTO(text=text, metadata=metadata))
        await self._vector_store.upsert(doc.id, chunks, embeddings)
        return doc
```

### Database Patterns (PostgreSQL + pgvector)

**Query Optimization**:
```python
# GOOD: Select specific columns
stmt = select(DocumentModel.id, DocumentModel.title).where(
    DocumentModel.status == "active"
).limit(20)

# BAD: Select everything
stmt = select(DocumentModel)  # Pulls all columns including large text fields
```

**N+1 Query Prevention**:
```python
# BAD: N+1
docs = await repo.find_all()
for doc in docs:
    doc.author = await get_user(doc.author_id)  # N queries

# GOOD: Batch fetch
docs = await repo.find_all()
author_ids = [d.author_id for d in docs]
authors = await get_users(author_ids)  # 1 query
author_map = {a.id: a for a in authors}
for doc in docs:
    doc.author = author_map.get(doc.author_id)
```

**Transactions**:
```python
async with session.begin():
    doc = await repo.create(doc_data)
    await vector_store.upsert(doc.id, chunks, embeddings)
    # Both succeed or both roll back
```

**pgvector Similarity Search**:
```python
from pgvector.sqlalchemy import Vector

class EmbeddingModel(Base):
    __tablename__ = "embeddings"

    id: Mapped[int] = mapped_column(primary_key=True)
    document_id: Mapped[str] = mapped_column(ForeignKey("documents.id"))
    chunk_text: Mapped[str]
    embedding: Mapped[list[float]] = mapped_column(Vector(1536))

# Cosine similarity search
stmt = (
    select(EmbeddingModel)
    .order_by(EmbeddingModel.embedding.cosine_distance(query_embedding))
    .limit(top_k)
)
```

### API Design (When REST Layer Is Added)

```python
# Resource-based URLs
GET    /api/documents              # List documents
GET    /api/documents/:id          # Get single document
POST   /api/documents              # Ingest document
DELETE /api/documents/:id          # Delete document
POST   /api/query                  # RAG query endpoint

# Query parameters for filtering, sorting, pagination
GET    /api/documents?status=active&sort=created_at&limit=20&offset=0
```

**Standard HTTP Status Codes**:
- `200` — Success
- `201` — Created
- `400` — Bad Request (validation errors)
- `401` — Unauthorized
- `403` — Forbidden
- `404` — Not Found
- `422` — Unprocessable Entity (semantic validation)
- `429` — Rate Limited
- `500` — Internal Server Error

### Caching (When Needed)

```python
from functools import lru_cache

@lru_cache(maxsize=128)
def get_embedding_model(model_name: str) -> BaseEmbedder:
    """Cache expensive model loading."""
    ...

# For query-level caching, use a simple TTL dict or Redis when added
```

---

## RAG Engine Design Principles

> These principles guide architectural decisions. Follow them unless the codebase explicitly deviates.

### Modularity & Extensibility

- Each RAG component (chunking, embedding, retrieval, generation) should be **pluggable**
- Use **abstract base classes or Protocols** to define interfaces for swappable backends
- New vector stores, LLM providers, or embedding models should be addable without modifying core logic

### Performance

- **Batch operations** where possible (batch embedding, batch retrieval)
- **Async support** where applicable (LLM calls, vector store queries, DB operations)
- **Lazy loading** of heavy dependencies (model loading, index initialization)
- **Connection pooling** for PostgreSQL (use SQLAlchemy async engine with pool settings)
- Profile before optimizing — don't prematurely optimize

### Data Flow

- Clear, traceable data pipeline: **Document → Chunks → Embeddings → Index (pgvector) → Query → Retrieval → Generation**
- Each stage should be independently testable and configurable
- Use well-defined data models (Pydantic for DTOs, SQLAlchemy for persistence) for inter-component communication

### Algorithmic Craftsmanship

> Inspired by generative art principles — applied to RAG pipeline design.

- **Process over product**: The pipeline's elegance matters. Each run should be traceable, reproducible, and debuggable
- **Parametric expression**: Expose meaningful configuration knobs (chunk size, overlap, top-k, similarity threshold) — not implementation details
- **Emergent quality**: Simple, composable components that produce sophisticated results when combined
- **Reproducibility**: Same inputs + same configuration = same outputs. Use deterministic operations where possible; seed randomness when it must exist
- **Expert refinement**: Tune parameters carefully. Don't settle for default values — profile and adjust for the specific use case

---

## Error Handling Patterns

### Custom Exception Hierarchy

```python
class VectorForgeError(Exception):
    """Base exception for all VectorForge errors."""

class ConfigurationError(VectorForgeError):
    """Invalid or missing configuration."""

class EmbeddingError(VectorForgeError):
    """Failed to generate embeddings."""

class RetrievalError(VectorForgeError):
    """Failed to retrieve documents from vector store."""

class LLMError(VectorForgeError):
    """Failed to generate LLM response."""

class DatabaseError(VectorForgeError):
    """Database operation failed."""
```

### Standard Approach

- Catch specific exceptions, never bare `except:`
- Provide **meaningful error messages** with context (what failed, what was expected, what was received)
- **Fail fast** on configuration errors; **retry with backoff** on transient API failures

### Retry with Backoff (for External Services)

```python
import asyncio

async def retry_with_backoff(
    fn,
    max_retries: int = 3,
    base_delay: float = 1.0,
):
    """Retry a coroutine with exponential backoff."""
    last_error: Exception | None = None
    for attempt in range(max_retries):
        try:
            return await fn()
        except Exception as e:
            last_error = e
            if attempt < max_retries - 1:
                delay = base_delay * (2 ** attempt)
                await asyncio.sleep(delay)
    raise last_error
```

### Logging Standards

- Use Python's `logging` module (not `print()`)
- **Structured logging** with consistent context (component, operation, duration)
- **Never log sensitive data** (API keys, user content in production)
- Log levels: `DEBUG` for internals, `INFO` for operations, `WARNING` for recoverable issues, `ERROR` for failures
- **Include the function/module name** where the log originated

```python
import logging

logger = logging.getLogger(__name__)

async def search(query: str, top_k: int = 5) -> list[SearchResult]:
    logger.info("Starting search", extra={"query_length": len(query), "top_k": top_k})
    try:
        results = await vector_store.search(query, top_k)
        logger.info("Search completed", extra={"result_count": len(results)})
        return results
    except RetrievalError as e:
        logger.error("Search failed", extra={"error": str(e)}, exc_info=True)
        raise
```

---

## Code Review Standards

### Review Priority Order

1. **Correctness & Bugs** (CRITICAL): Logic errors, race conditions, edge cases, security vulnerabilities
2. **SOLID Compliance** (HIGH): Evaluate against all 5 principles (see SOLID section above)
3. **DRY** (HIGH): Identify service-level logic duplication
4. **Complexity Reduction** (MEDIUM): Can a simpler construct replace this?
5. **Readability** (MEDIUM): Are variable naming and flow self-explanatory?

### Review Constraints (What NOT To Do)

- Do **NOT** comment on trivial formatting (whitespace, indentation) unless it breaks the build
- Do **NOT** compliment the code — focus only on improvements
- Do **NOT** suggest refactoring if the current implementation is standard and readable

### Review Checklist

- [ ] **Correctness** — Logic errors, edge cases, boundary conditions
- [ ] **Type Safety** — Complete type hints, correct types, type consistency
- [ ] **Error Handling** — Specific exceptions, informative messages, no bare except
- [ ] **Performance** — Inefficient loops, N+1 queries, unnecessary computations
- [ ] **Security** — SQL injection (use parameterized queries), user input validation, no hardcoded secrets
- [ ] **SOLID** — Single responsibility, open/closed, dependency inversion
- [ ] **DRY** — Duplicated logic at service level
- [ ] **Documentation** — Docstrings, clear comments for complex logic only
- [ ] **Testing** — Missing test cases, inadequate coverage, unmocked externals

### Review Output Format

```markdown
## Critical Issues

1. **[Issue Name]** (file.py:L42)
   - **Problem:** What's wrong
   - **Impact:** What breaks / what's the risk
   - **Fix:** Concrete code fix

## High Priority

1. ...

## Recommendations
- ...
```

---

## Frontend Standards (React — Future)

> These standards apply only when the React UI layer is added. Until then, do NOT create frontend code.

### When Activated

- **Framework**: React + TypeScript + Vite
- **Styling**: Tailwind CSS (or as chosen when UI work begins)
- **State Management**: React Query for server state, Context/Zustand for client state
- **HTTP**: Axios or fetch with typed API client
- **Routing**: React Router

### Design Rules

- **No hardcoded text strings** — use i18n keys (`t('actions.submit')`)
- **No hardcoded colors** — use theme variables / Tailwind semantic classes
- **WCAG 2.2 AA compliant** — color contrast, focus states, ARIA labels
- **DRY components** — extract common patterns into reusable components immediately
- **No new npm packages** without approval — use existing dependencies or implement with standard React/HTML/CSS

### Component Architecture

```
frontend/
├── src/
│   ├── components/       # Reusable UI components
│   │   ├── ui/           # Base components (Button, Input)
│   │   └── features/     # Feature-specific components
│   ├── hooks/            # Custom React hooks
│   ├── api/              # API client and types
│   ├── types/            # TypeScript types
│   ├── utils/            # Helper functions
│   └── pages/            # Page-level components
```

---

## Response Format (When You Have Context)

1. **Summary** — brief overview of what you're implementing
2. **References** — exact file paths, APIs, schemas you're using
3. **Plan** — step-by-step implementation approach
4. **Changes** — code diffs or complete code blocks
5. **Validation** — how to test, edge cases to consider
6. **Risks & Follow-ups** — potential issues, future improvements

---

## Troubleshooting

### Common Issues

- **Import errors**: Check virtual environment activation and package installation
- **Test failures**: Verify mocks, fixtures, and test data setup
- **Type errors**: Run mypy and check type annotations
- **Dependency conflicts**: Check `pyproject.toml` version constraints
- **Database errors**: Check PostgreSQL connection, pgvector extension, migrations
- **Embedding dimension mismatch**: Verify model output dimensions match pgvector column definition

### When Stuck

1. **Review similar implementations** in the codebase
2. **Check module README or docstrings** for guidance
3. **Examine test files** for usage examples
4. **Ask specific questions** rather than making assumptions

---

## Notes & Best Practices

1. **Mirror the codebase design exactly** — don't invent new patterns unless they improve on existing ones; suggest improvements and ask before implementing
2. **If multiple solutions exist**, ask before choosing
3. **Use existing shared utilities** instead of re-implementing
4. **Optimize for readability + maintainability**
5. **When unsure about design/data flow/integrations** — ASK FIRST
6. **Never assume external dependencies exist** unless confirmed in repo
7. **Keep solutions simple and idiomatic** Python
8. **This project targets standalone usage** — avoid coupling to specific cloud providers unless explicitly designed for it
9. **PostgreSQL is the primary store** — do not suggest alternative databases without being asked
10. **pgvector is the vector backend** — do not suggest Pinecone/Weaviate/ChromaDB unless explicitly requested

---

## Correction Log — Mistakes & Required Behavior

> **Purpose**: This section records specific mistakes made during development and the correct behavior to follow when that scenario arises again. Every time feedback highlights an error, a new entry is added here. This log is **append-only** — entries are never removed.
>
> **Format**:
> - **Scenario**: What happened / what was the situation
> - **Mistake**: What was incorrectly done
> - **Correct Behavior**: What should have been done instead
> - **Date**: When the correction was recorded

<!-- CORRECTION LOG START — append new entries below this line -->

_No corrections recorded yet. Entries will be added as feedback is received._

<!-- CORRECTION LOG END -->

---

**Remember: VectorForge prioritizes reliability, performance, and clean architecture. Take time to understand existing patterns before implementing new features.**
